# NX Tensors

```elixir
Mix.install([
  {:nx, "~> 0.5"},
  {:exla, "~> 0.5"},
  {:benchee, "~> 1.0"}
])
```

## Intro to Tensor

```elixir
a = Nx.tensor([[1, 2, 3], [4, 5, 6]])
b = Nx.tensor(1.0)
c = Nx.tensor([[[[[1.0, 2]]]]])
IO.inspect(a, label: :a)
IO.inspect(b, label: :b)
IO.inspect(c, label: :c)

Nx.tensor(0.000000000000000000000000000000000000000000000001, type: {:f, 64})
Nx.tensor(128, type: {:s, 8})
Nx.tensor([1.0, 2, 3])
Nx.tensor(10)
Nx.tensor([[1, 2, 3], [4, 5, 6]], names: [:x, :y])

Nx.to_binary(a)
```

```elixir
<<1::64-signed-native, 2::64-signed-native, 3::64-signed-native>>
|> Nx.from_binary({:s, 64})
|> Nx.reshape({1, 3})
```

## Nx Operations

```elixir
a = Nx.tensor([1, 2, 3])

a
|> Nx.as_type({:f, 32})
|> Nx.reshape({1, 3, 1})

Nx.bitcast(a, {:f, 64})

a = Nx.tensor([[[-1, -2, -3], [-4, -5, -6]], [[1, 2, 3], [4, 5, 6]]])
Nx.abs(a)

a = Nx.tensor([[1, 2, 3], [4, 5, 6]])
b = Nx.tensor([[6, 7, 8], [9, 10, 11]])

Nx.add(a, b)

Nx.multiply(a, b)

Nx.add(5, Nx.tensor([1, 2, 3]))

Nx.add(Nx.tensor([1, 2, 3]), Nx.tensor([[4, 5, 6], [7, 8, 9]]))

revs = Nx.tensor([85, 76, 42, 34, 46, 23, 52, 99, 22, 32, 85, 51])

Nx.sum(revs)

revs =
  Nx.tensor(
    [
      [21, 64, 86, 26, 74, 81, 38, 79, 70, 48, 85, 33],
      [64, 82, 48, 39, 70, 71, 81, 53, 50, 67, 36, 50],
      [68, 74, 39, 78, 95, 62, 53, 21, 43, 59, 51, 88],
      [47, 74, 97, 51, 98, 47, 61, 36, 83, 55, 74, 43]
    ],
    names: [:year, :month]
  )

Nx.sum(revs, axes: [:year])

Nx.sum(revs, axes: [:month])
```

## Defn

```elixir
defmodule MyModule do
  import Nx.Defn

  defn adds_one(x) do
    Nx.add(x, 1)
    |> print_expr()
  end
end

MyModule.adds_one(Nx.tensor([1, 2, 3]))
```

## Section

```elixir
defmodule Softmax do
  import Nx.Defn

  defn(softmax(n), do: Nx.exp(n) / Nx.sum(Nx.exp(n)))
end

Nx.default_backend(EXLA.Backend)
key = Nx.Random.key(1_000_000)
{tensor, _new_key} = Nx.Random.uniform(key)

Benchee.run(
  %{
    "JIT with EXLA" => fn ->
      apply(EXLA.jit(&Softmax.softmax/1), [tensor])
    end,
    "Regular Elixir" => fn ->
      Softmax.softmax(tensor)
    end
  },
  time: 10
)
```
